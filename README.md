# Gloss-Free, Semi-Supervised Sign Language Recognition Using Webcam Data

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A complete, production-ready deep learning system for real-time sign language recognition using webcam input, featuring semi-supervised learning with pseudo-labeling and multimodal fusion.

## ‚ú® Features

- üéØ **Gloss-Free Learning**: Direct sign-to-text mapping without intermediate gloss representations
- üîÑ **Semi-Supervised Learning**: Leverage large unlabeled datasets with pseudo-labeling
- üé• **Multimodal Inputs**: RGB frames + hand landmarks + pose landmarks via MediaPipe
- ‚ö° **Real-Time Inference**: <50ms latency with sliding window approach
- üì¶ **Model Export**: TorchScript, ONNX, and quantized models for deployment
- üéì **Ready to Use**: Complete pipeline from data collection to deployment

## üèóÔ∏è Architecture

```
Input ‚Üí CNN Encoder (ResNet18/MobileNet) ‚Üí Transformer Temporal Encoder
     ‚Üí Landmark Encoders ‚Üí Attention Fusion ‚Üí Classifier ‚Üí Output
```

**Components:**
- **CNN Encoder**: ResNet18 or MobileNetV2 (pretrained on ImageNet)
- **Transformer**: 4-layer temporal encoder with multi-head attention
- **Fusion**: Attention-based multimodal fusion
- **Landmarks**: MediaPipe hand and pose estimation

## üöÄ Quick Start

### Prerequisites
- Python 3.8+ with pip
- CUDA-capable GPU (for training)
- Webcam (for inference)
- ~25GB free disk space for dataset

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/D4AX7/Gloss-Free-Semi-Supervised-Sign-Language-Recognition-Using-Webcam-Data.git
cd Gloss-Free-Semi-Supervised-Sign-Language-Recognition-Using-Webcam-Data
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Download ASL Alphabet Dataset**
- Download from [Kaggle ASL Alphabet](https://www.kaggle.com/datasets/grassknoted/asl-alphabet)
- Extract to `data/raw/` directory
- Should contain folders: A-Z, HELLO, THANKYOU, YES, NO, SORRY

4. **Generate Training Data**
```bash
python convert_full_dataset.py
```
This creates:
- `data/labeled_new/train_data.npz` (18.31 GB)
- `data/labeled_new/val_data.npz` (4.58 GB)

### Training

**For GPU Training** (Recommended):
```bash
START_TRAINING.bat
```
- Trains for 10 epochs (~2-3 hours on modern GPU)
- Saves checkpoints to `models/`
- Monitor with TensorBoard: `tensorboard --logdir=runs`

**Manual Training**:
```bash
python src/train.py
```

### Testing

Test with webcam:
```bash
TEST_NEW_MODEL.bat
```

Or run directly:
```bash
python src/infer_simple.py
```

```bash
python src/infer.py
```

## üìñ Documentation

- **[INSTALLATION.md](INSTALLATION.md)**: Complete installation and setup guide
- **[TRAINING_GUIDE.md](TRAINING_GUIDE.md)**: Detailed training instructions
- **[API_DOCUMENTATION.md](API_DOCUMENTATION.md)**: Complete API reference
- **[PROJECT_SUMMARY.md](PROJECT_SUMMARY.md)**: Comprehensive project overview

## üìä Project Structure

```
signssl-project/
‚îú‚îÄ‚îÄ README.md                     # This file
‚îú‚îÄ‚îÄ API_DOCUMENTATION.md         # API reference
‚îú‚îÄ‚îÄ LICENSE                      # MIT License
‚îú‚îÄ‚îÄ requirements.txt             # Dependencies
‚îú‚îÄ‚îÄ .gitignore                   # Git ignore rules
‚îÇ
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml             # Training configuration
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # ASL Alphabet dataset (not in git - download separately)
‚îÇ   ‚îú‚îÄ‚îÄ labeled_new/            # Processed training data (not in git)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_data.npz      # 18.31 GB - 8,107 samples
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ val_data.npz        # 4.58 GB - 2,027 samples
‚îÇ   ‚îî‚îÄ‚îÄ label_mapping.json      # Label mappings
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ collect_data.py         # Data collection
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py           # Preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py              # PyTorch datasets (memory-mapped loading)
‚îÇ   ‚îú‚îÄ‚îÄ model.py                # ResNet18 + Transformer architecture
‚îÇ   ‚îú‚îÄ‚îÄ train.py                # Training script
‚îÇ   ‚îú‚îÄ‚îÄ infer_simple.py         # Real-time webcam inference
‚îÇ   ‚îú‚îÄ‚îÄ export.py               # Model export
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                # Utilities
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint.pth          # Latest checkpoint (not in git - train locally)
‚îÇ   ‚îî‚îÄ‚îÄ best_model.pth          # Best model (not in git - train locally)
‚îÇ   ‚îú‚îÄ‚îÄ quantized_model.pth     # Quantized model
‚îÇ   ‚îú‚îÄ‚îÄ model_scripted.pt       # TorchScript export
‚îÇ   ‚îî‚îÄ‚îÄ model.onnx              # ONNX export
‚îÇ
‚îÇ
‚îú‚îÄ‚îÄ convert_full_dataset.py     # Convert ASL Alphabet to training data
‚îú‚îÄ‚îÄ START_TRAINING.bat          # Windows training script
‚îî‚îÄ‚îÄ TEST_NEW_MODEL.bat          # Windows testing script
```

## ‚öôÔ∏è Configuration

Edit `configs/config.yaml` to customize training:

```yaml
data:
  frames_per_clip: 16           # Number of frames per clip
  frame_height: 224             # Frame height
  frame_width: 224              # Frame width
  train_data_path: data/labeled_new/train_data.npz
  val_data_path: data/labeled_new/val_data.npz

model:
  encoder_type: 'resnet18'      # CNN encoder
  hidden_dim: 512               # Hidden dimension
  num_heads: 8                  # Attention heads
  num_transformer_layers: 4     # Transformer layers
  num_classes: 31               # 30 ASL signs + background

training:
  batch_size: 16                # Batch size (reduce if OOM)
  learning_rate: 0.0001         # Learning rate
  epochs: 10                    # Training epochs
  mixed_precision: true         # Use AMP for faster training
  
inference:
  sliding_window_stride: 4      # Inference stride
  confidence_threshold: 0.7     # Display threshold
```

## üìà Performance

**Current Status (6 epochs trained):**
- Training Accuracy: 98.31%
- Validation Accuracy: ~95%
- Inference Confidence: 30-70% (undertrained)

**Expected with Full Training (10 epochs):**
- Training Accuracy: >99%
- Validation Accuracy: >95%
- Inference Confidence: 80-95% (production-ready)

**Dataset:**
- Total Samples: 10,134 (8,107 train, 2,027 validation)
- Source: ASL Alphabet dataset (26,000+ images)
- Classes: 30 ASL signs (A-Z + HELLO, THANKYOU, YES, NO, SORRY)
- Augmentation: Random rotation, brightness, contrast

## üõ†Ô∏è Advanced Usage

### Monitor Training with TensorBoard

```bash
tensorboard --logdir=runs
```

### Resume Training from Checkpoint

```bash
python src/train.py --resume models/checkpoint.pth
```

## üêõ Troubleshooting

**Out of Memory during Training?**
- Reduce `batch_size` in `configs/config.yaml` (try 8 or 4)
- Close other GPU applications
- Use smaller resolution (reduce to 160x160)

**Model predicting same sign repeatedly?**
- Model is undertrained - complete full 10 epochs
- Check dataset has balanced classes
- Verify data augmentation is working

**Low confidence predictions?**
- Train for more epochs (minimum 10)
- Use larger dataset if possible
- Check webcam lighting and hand positioning

**Dataset conversion taking too long?**
- Normal for 26,000 images - expect 30-60 minutes
- Progress bar shows current status
- Output files will be ~23GB total

## üìö Dataset

This project uses the **ASL Alphabet** dataset:
- **Source**: [Kaggle - ASL Alphabet](https://www.kaggle.com/datasets/grassknoted/asl-alphabet)
- **Size**: 87,000 images (26 letters + 4 phrases)
- **License**: CC0 Public Domain
- **Processing**: Converted to sequential frames with augmentation

## ü§ù Contributing

Contributions welcome! Areas for improvement:
- Complete 10-epoch training and share model weights
- Add more ASL signs and phrases
- Improve data augmentation strategies
- Optimize inference speed
- Mobile deployment (TFLite, Core ML)

## üìÑ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- **ASL Alphabet Dataset** by grassknoted on Kaggle
- **MediaPipe** by Google for hand landmark extraction
- **PyTorch** team for the deep learning framework
- Sign language research community

## üìß Contact

For questions and support, please open an issue on the [GitHub repository](https://github.com/D4AX7/Gloss-Free-Semi-Supervised-Sign-Language-Recognition-Using-Webcam-Data).

---

**Repository**: https://github.com/D4AX7/Gloss-Free-Semi-Supervised-Sign-Language-Recognition-Using-Webcam-Data

**Built with ‚ù§Ô∏è for accessibility and inclusion**
