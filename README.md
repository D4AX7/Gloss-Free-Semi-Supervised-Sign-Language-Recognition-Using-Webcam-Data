# Gloss-Free, Semi-Supervised Sign Language Recognition Using Webcam Data

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A complete, production-ready deep learning system for real-time sign language recognition using webcam input, featuring semi-supervised learning with pseudo-labeling and multimodal fusion.

## âœ¨ Features

- ðŸŽ¯ **Gloss-Free Learning**: Direct sign-to-text mapping without intermediate gloss representations
- ðŸ”„ **Semi-Supervised Learning**: Leverage large unlabeled datasets with pseudo-labeling
- ðŸŽ¥ **Multimodal Inputs**: RGB frames + hand landmarks + pose landmarks via MediaPipe
- âš¡ **Real-Time Inference**: <50ms latency with sliding window approach
- ðŸ“¦ **Model Export**: TorchScript, ONNX, and quantized models for deployment
- ðŸŽ“ **Ready to Use**: Complete pipeline from data collection to deployment

## ðŸ—ï¸ Architecture

```
Input â†’ CNN Encoder (ResNet18/MobileNet) â†’ Transformer Temporal Encoder
     â†’ Landmark Encoders â†’ Attention Fusion â†’ Classifier â†’ Output
```

**Components:**
- **CNN Encoder**: ResNet18 or MobileNetV2 (pretrained on ImageNet)
- **Transformer**: 4-layer temporal encoder with multi-head attention
- **Fusion**: Attention-based multimodal fusion
- **Landmarks**: MediaPipe hand and pose estimation

## ðŸš€ Quick Start

### Automated Setup (Recommended)

**Windows:**
```bash
quick_start.bat
```

**All Platforms:**
```bash
python setup_and_run.py
```

### Manual Setup

#### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

#### 2. Generate Synthetic Data (for testing)

```bash
python examples/generate_synthetic_data.py --samples 20
```

#### 3. Preprocess Data

```bash
python src/preprocess.py
```

#### 4. Create Initial Models

```bash
python create_models.py
```

#### 5. Train Model

```bash
python src/train.py
```

#### 6. Pseudo-Label Unlabeled Data (Semi-Supervised)

```bash
python src/pseudo_label.py
```

#### 7. Export Models

```bash
python src/export.py
```

#### 8. Real-Time Inference

```bash
python src/infer.py
```

## ðŸ“– Documentation

- **[INSTALLATION.md](INSTALLATION.md)**: Complete installation and setup guide
- **[TRAINING_GUIDE.md](TRAINING_GUIDE.md)**: Detailed training instructions
- **[API_DOCUMENTATION.md](API_DOCUMENTATION.md)**: Complete API reference
- **[PROJECT_SUMMARY.md](PROJECT_SUMMARY.md)**: Comprehensive project overview

## ðŸŽ¯ Collecting Real Data

```bash
# Collect labeled data for different signs
python src/collect_data.py --label hello --samples 50
python src/collect_data.py --label thanks --samples 50
python src/collect_data.py --label yes --samples 50
# ... add more signs

# Collect unlabeled data for semi-supervised learning
python src/collect_data.py --label unknown --samples 100 --unlabeled
```

## ðŸ“Š Project Structure

```
signssl-project/
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ INSTALLATION.md              # Installation guide
â”œâ”€â”€ TRAINING_GUIDE.md            # Training instructions
â”œâ”€â”€ API_DOCUMENTATION.md         # API reference
â”œâ”€â”€ PROJECT_SUMMARY.md           # Complete overview
â”œâ”€â”€ LICENSE                      # MIT License
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml             # Configuration file
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Raw data
â”‚   â”œâ”€â”€ labeled/                # Labeled data
â”‚   â”œâ”€â”€ unlabeled/              # Unlabeled data
â”‚   â”œâ”€â”€ processed_labeled.npz   # Processed labeled data
â”‚   â”œâ”€â”€ processed_unlabeled.npz # Processed unlabeled data
â”‚   â””â”€â”€ label_mapping.json      # Label mappings
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ collect_data.py         # Data collection
â”‚   â”œâ”€â”€ preprocess.py           # Preprocessing
â”‚   â”œâ”€â”€ dataset.py              # PyTorch datasets
â”‚   â”œâ”€â”€ model.py                # Model architecture
â”‚   â”œâ”€â”€ train.py                # Training script
â”‚   â”œâ”€â”€ pseudo_label.py         # Pseudo-labeling
â”‚   â”œâ”€â”€ infer.py                # Real-time inference
â”‚   â”œâ”€â”€ export.py               # Model export
â”‚   â””â”€â”€ utils.py                # Utilities
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_model.pth          # Best trained model
â”‚   â”œâ”€â”€ quantized_model.pth     # Quantized model
â”‚   â”œâ”€â”€ model_scripted.pt       # TorchScript export
â”‚   â””â”€â”€ model.onnx              # ONNX export
â”‚
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ generate_synthetic_data.py  # Generate test data
â”‚
â”œâ”€â”€ create_models.py            # Create initial models
â”œâ”€â”€ setup_and_run.py            # Automated setup
â”œâ”€â”€ quick_start.bat             # Windows quick start
â””â”€â”€ test_system.py              # System tests
```

## âš™ï¸ Configuration

Edit `configs/config.yaml` to customize:

```yaml
data:
  frames_per_clip: 16           # Number of frames per clip
  frame_height: 224             # Frame height
  frame_width: 224              # Frame width

model:
  encoder_type: 'resnet18'      # 'resnet18' or 'mobilenet_v2'
  hidden_dim: 512               # Hidden dimension
  num_heads: 8                  # Attention heads
  num_transformer_layers: 4     # Transformer layers

training:
  batch_size: 8                 # Batch size
  learning_rate: 0.0001         # Learning rate
  epochs: 50                    # Training epochs
  mixed_precision: true         # Use AMP

semi_supervised:
  pseudo_label_threshold: 0.9   # Confidence threshold
  
inference:
  sliding_window_stride: 4      # Inference stride
  confidence_threshold: 0.7     # Display threshold
```

## ðŸ§ª Testing

Run system tests to verify installation:

```bash
python test_system.py
```

## ðŸ“ˆ Performance

**Expected Results:**
- Training Accuracy: >90% (with sufficient data)
- Validation Accuracy: >80%
- Inference Speed: <50ms per prediction
- Model Size: ~20-50MB (quantized: ~10-20MB)

## ðŸ› ï¸ Advanced Usage

### Monitor Training with TensorBoard

```bash
tensorboard --logdir logs/train
```

### Use Quantized Model for Faster Inference

```bash
python src/infer.py --model models/quantized_model.pth
```

### Export to ONNX for Deployment

```bash
python src/export.py --formats onnx
```

## ðŸ› Troubleshooting

**Out of Memory?**
- Reduce `batch_size` in config.yaml
- Use `mobilenet_v2` encoder
- Disable mixed precision

**Low Accuracy?**
- Collect more training data (50+ samples per sign)
- Increase training epochs
- Use pseudo-labeling

**Slow Inference?**
- Use quantized model
- Reduce `frames_per_clip`
- Increase `sliding_window_stride`

See [INSTALLATION.md](INSTALLATION.md) for more troubleshooting tips.

## ðŸ“š Citation

If you use this project in your research, please cite:

```bibtex
@misc{signssl2024,
  title={Gloss-Free, Semi-Supervised Sign Language Recognition},
  author={Sign SSL Project},
  year={2024},
  howpublished={\url{https://github.com/yourusername/signssl-project}}
}
```

## ðŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ðŸ™ Acknowledgments

- MediaPipe for landmark extraction
- PyTorch team for the framework
- Sign language research community

## ðŸ“§ Contact

For questions and support, please open an issue on GitHub.

---

**Built with â¤ï¸ for accessibility and inclusion**
